{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcf4f8b2-31b3-4c59-96a7-f2f5a88a5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../src\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ccca65b-f256-4dea-993d-6232596d5235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 08:17:24.662524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741940244.683026    1937 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741940244.690589    1937 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-14 08:17:24.725403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import argparse\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pose_estimator import PoseEstimator3D\n",
    "from kalman_filter import KalmanFilterTracker3D\n",
    "from skeleton_visualizer import SkeletonVisualizer\n",
    "from kalman_filter import KeypointPreprosess\n",
    "from com_calculator import COMCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851cf38e-4fde-4c8e-b415-6b61a5913658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import memory_allocated, empty_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d511128-0668-4161-ab14-42fc4003ded1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() == True:\n",
    "    device = 'cuda:0'\n",
    "    print('GPU available')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('GPU not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c4477b-d7fa-4657-823f-82a293a42de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Deep_LSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=69, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=256, hidden_size=512, num_layers=1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.lstm4 = nn.LSTM(input_size=512, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        self.lstm5 = nn.LSTM(input_size=256, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.lstm6 = nn.LSTM(input_size=128, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.lstm7 = nn.LSTM(input_size=64, hidden_size=32, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(32,4)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x, _ = self.lstm5(x)\n",
    "        x, _ = self.lstm6(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm7(x)\n",
    "        x = self.fc(x[:,-1,:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8029637c-8f0b-47d6-9404-76d3950e72c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deep_LSTM(\n",
       "  (lstm1): LSTM(69, 128, batch_first=True)\n",
       "  (lstm2): LSTM(128, 256, batch_first=True)\n",
       "  (lstm3): LSTM(256, 512, batch_first=True)\n",
       "  (dropout1): Dropout(p=0.3, inplace=False)\n",
       "  (lstm4): LSTM(512, 256, batch_first=True)\n",
       "  (lstm5): LSTM(256, 128, batch_first=True)\n",
       "  (lstm6): LSTM(128, 64, batch_first=True)\n",
       "  (dropout2): Dropout(p=0.3, inplace=False)\n",
       "  (lstm7): LSTM(64, 32, batch_first=True)\n",
       "  (fc): Linear(in_features=32, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Deep_LSTM()\n",
    "net.to(device)\n",
    "net.load_state_dict(torch.load('model.pth'))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78a2e8d8-61ee-4f3a-852f-b8c2e6ec164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceVideo:\n",
    "    def __init__(self, roi_ratio=0.8, video_path=None):\n",
    "        self.pose_estimator = PoseEstimator3D(roi_ratio=roi_ratio)\n",
    "        self.kalman_tracker = KalmanFilterTracker3D()\n",
    "        self.skeleton_visualizer = SkeletonVisualizer()\n",
    "        self.com_calculator = COMCalculator()\n",
    "        self.keypoint_preprosessing = KeypointPreprosess(True)\n",
    "        \n",
    "        self.video_path = video_path\n",
    "        self.com_trajectory = []\n",
    "        self.max_trajectory_length = 60\n",
    "        \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        프레임 처리 및 3D 포즈 추적\n",
    "        Args:\n",
    "            frame (numpy.ndarray): 입력 프레임\n",
    "        Returns:\n",
    "            numpy.ndarray: 추적 결과가 표시된 프레임\n",
    "        \"\"\"\n",
    "        try:\n",
    "            landmarks, processed_frame = self.pose_estimator.estimate_pose(frame)\n",
    "            \n",
    "            if landmarks:\n",
    "                # 3D 키포인트 추출\n",
    "                keypoints_3d = self.pose_estimator.extract_3d_keypoints(landmarks, frame)\n",
    "                \n",
    "                # 칼만 필터로 키포인트 필터링\n",
    "                filtered_keypoints_3d = self.kalman_tracker.track(keypoints_3d)\n",
    "                \n",
    "                # 3D CoM 계산\n",
    "                com_3d = self.com_calculator.calculate_whole_body_com(filtered_keypoints_3d, include_z=True)\n",
    "                \n",
    "                if com_3d:\n",
    "                    # CoM 궤적 업데이트\n",
    "                    self.com_trajectory.append(com_3d)\n",
    "                    if len(self.com_trajectory) > self.max_trajectory_length:\n",
    "                        self.com_trajectory.pop(0)\n",
    "                        \n",
    "                    # 현재 CoM 표시\n",
    "                    cv2.circle(processed_frame, (int(com_3d[0]), int(com_3d[1])), 8, (0, 255, 255), -1)\n",
    "                    cv2.putText(processed_frame, f\"CoM z:{int(com_3d[2])}\",\n",
    "                                (int(com_3d[0]) + 10, int(com_3d[1]) - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n",
    "                    \n",
    "                    # CoM 궤적 그리기\n",
    "                    for i in range(1, len(self.com_trajectory)):\n",
    "                        prev_com = self.com_trajectory[i-1]\n",
    "                        curr_com = self.com_trajectory[i]\n",
    "                        \n",
    "                        # 색상 그라데이션 (오래된 포인트일수록 더 투명하게)\n",
    "                        alpha = i / len(self.com_trajectory)\n",
    "                        \n",
    "                        # Z 값에 따른 색상 변화 (깊이 시각화)\n",
    "                        z_val = curr_com[2]\n",
    "                        z_color = (\n",
    "                            0,\n",
    "                            int(255 * alpha),\n",
    "                            int(255 * (1.0 - abs(z_val) / 100) * alpha)  # Z값에 따른 색상 변화\n",
    "                        )\n",
    "                        \n",
    "                        cv2.line(processed_frame,\n",
    "                                 (int(prev_com[0]), int(prev_com[1])),\n",
    "                                 (int(curr_com[0]), int(curr_com[1])),\n",
    "                                 z_color, 2)\n",
    "                        \n",
    "                    # 3D 이동 방향 계산 및 시각화\n",
    "                    if len(self.com_trajectory) >= 5:\n",
    "                        direction, speed = self.com_calculator.calculate_movement_direction(\n",
    "                            self.com_trajectory, window_size=5)\n",
    "                        \n",
    "                        # 방향 시각화\n",
    "                        processed_frame = self.skeleton_visualizer.draw_direction_arrow(\n",
    "                            processed_frame, com_3d, direction, speed)\n",
    "                        \n",
    "                # 3D 스켈레톤 시각화\n",
    "                processed_frame = self.skeleton_visualizer.draw_3d_skeleton(\n",
    "                    processed_frame, keypoints_3d, filtered_keypoints_3d)\n",
    "                \n",
    "                keypoints_and_com = filtered_keypoints_3d[11:]\n",
    "                keypoints_and_com.append(com_3d)\n",
    "                \n",
    "                keypoints_and_com = self.keypoint_preprosessing.prosess(keypoints_and_com)\n",
    "                sequence_data = [x for y in keypoints_and_com for x in y]\n",
    "                \n",
    "            return sequence_data, processed_frame\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return frame\n",
    "            \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        비디오 캡처 및 처리\n",
    "        \n",
    "        MP4 파일을 프레임 수에 따라 라벨링 후 병합\n",
    "        \"\"\"\n",
    "        # 비디오 파일을 사용하여 캡처\n",
    "        if not self.video_path:\n",
    "            print(\"Error: No video source provided.\")\n",
    "            return\n",
    "            \n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        sequence_tensor, current_frame = [[] , 20]\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open video source.\")\n",
    "            return\n",
    "            \n",
    "        width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps    = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        \n",
    "        out_video_name = self.video_path.rsplit('.')[0] + \"_out.mp4\"\n",
    "        out = cv2.VideoWriter(out_video_name, fourcc, fps, (width, height))\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                print(\"End of video or failed to capture frame\")\n",
    "                break\n",
    "\n",
    "            if len(sequence_tensor) < current_frame:\n",
    "                sequence_vector, visual_frame = self.process_frame(frame)\n",
    "                sequence_tensor.append(sequence_vector)\n",
    "            else :\n",
    "                sequence_tensor = sequence_tensor[1:]\n",
    "                sequence_vector, visual_frame = self.process_frame(frame)\n",
    "                sequence_tensor.append(sequence_vector)\n",
    "            \n",
    "            if len(sequence_tensor) == current_frame:\n",
    "                data = torch.tensor(sequence_tensor, dtype=torch.float32).unsqueeze(0)\n",
    "                data = data.to(device)\n",
    "                with torch.no_grad():\n",
    "                    result = net(data)\n",
    "                    _, outs = torch.max(result, 1)\n",
    "                    if outs.item() == 0 : status = 'Stand'\n",
    "                    elif outs.item() == 1 : status = 'Start Walking'\n",
    "                    elif outs.item() == 2 : status = 'During Walking'\n",
    "                    else : status = 'Finish Walking'\n",
    "            else :\n",
    "                status = 'None'\n",
    "            \n",
    "            cv2.putText(visual_frame, status, (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 0, 255), 2)\n",
    "\n",
    "            if isinstance(visual_frame, torch.Tensor):\n",
    "                visual_frame = visual_frame.detach().cpu().numpy()\n",
    "                if visual_frame.ndim == 3:  # (C, H, W)\n",
    "                    visual_frame = np.transpose(visual_frame, (1, 2, 0))\n",
    "                visual_frame = (visual_frame * 255).astype(np.uint8)\n",
    "            \n",
    "            out.write(visual_frame)\n",
    "            \n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f29c6c26-82ba-4d48-aa4a-251e5acdd1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1741942428.990175    2470 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1741942429.076170    2470 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "inference = InferenceVideo(roi_ratio=1, video_path='testtesttest.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c1591fb-2d1c-4a07-9d1c-d78af32142d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of video or failed to capture frame\n"
     ]
    }
   ],
   "source": [
    "inference.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
